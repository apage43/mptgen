{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "mpath = Path.home() / 'model' / 'mpt-7b-chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron/conv-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /Users/aaron/.cache/huggingface/modules/transformers_modules/mpt-7b-chat/modeling_mpt.py\n",
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  mpath,\n",
    "  trust_remote_code=True,\n",
    "  low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPTForCausalLM(\n",
       "  (transformer): MPTModel(\n",
       "    (wte): SharedEmbedding(50432, 4096)\n",
       "    (emb_drop): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aaron/model/ggml-mpt-7b-chat-vocabless-f16.bin\n"
     ]
    }
   ],
   "source": [
    "ftype_str = [\"f32\", \"f16\"]\n",
    "ftype = 1\n",
    "fname_out = Path.home() / 'model' / (f\"ggml-{mpath.name}-vocabless-\" + ftype_str[ftype] + \".bin\")\n",
    "print(fname_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPTConfig {\n",
      "  \"_name_or_path\": \"/Users/aaron/model/mpt-7b-chat\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"sam-mosaic/gpt-neox-20b-chatml\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.29.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "assert(model.config.attn_config['alibi'])\n",
    "assert(model.config.no_bias)\n",
    "assert(not model.config.attn_config['prefix_lm'])\n",
    "assert(model.config.norm_type == \"low_precision_layernorm\")\n",
    "assert(not model.config.attn_config['qk_ln'])\n",
    "assert(model.config.expansion_ratio == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable: transformer.wte.weight with shape:  (50432, 4096)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.0.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.0.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.0.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.0.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.0.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.0.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.1.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.1.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.1.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.1.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.1.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.1.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.2.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.2.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.2.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.2.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.2.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.2.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.3.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.3.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.3.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.3.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.3.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.3.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.4.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.4.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.4.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.4.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.4.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.4.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.5.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.5.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.5.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.5.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.5.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.5.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.6.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.6.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.6.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.6.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.6.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.6.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.7.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.7.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.7.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.7.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.7.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.7.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.8.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.8.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.8.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.8.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.8.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.8.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.9.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.9.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.9.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.9.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.9.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.9.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.10.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.10.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.10.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.10.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.10.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.10.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.11.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.11.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.11.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.11.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.11.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.11.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.12.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.12.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.12.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.12.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.12.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.12.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.13.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.13.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.13.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.13.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.13.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.13.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.14.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.14.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.14.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.14.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.14.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.14.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.15.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.15.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.15.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.15.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.15.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.15.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.16.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.16.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.16.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.16.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.16.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.16.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.17.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.17.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.17.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.17.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.17.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.17.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.18.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.18.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.18.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.18.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.18.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.18.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.19.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.19.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.19.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.19.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.19.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.19.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.20.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.20.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.20.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.20.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.20.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.20.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.21.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.21.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.21.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.21.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.21.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.21.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.22.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.22.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.22.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.22.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.22.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.22.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.23.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.23.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.23.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.23.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.23.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.23.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.24.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.24.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.24.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.24.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.24.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.24.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.25.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.25.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.25.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.25.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.25.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.25.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.26.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.26.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.26.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.26.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.26.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.26.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.27.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.27.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.27.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.27.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.27.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.27.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.28.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.28.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.28.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.28.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.28.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.28.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.29.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.29.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.29.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.29.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.29.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.29.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.30.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.30.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.30.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.30.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.30.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.30.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.31.norm_1.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.31.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.31.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.31.norm_2.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "Processing variable: transformer.blocks.31.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.blocks.31.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "  Converting to float16\n",
      "Processing variable: transformer.norm_f.weight with shape:  (4096,)\n",
      "  Converting to float32\n",
      "/Users/aaron/model/ggml-mpt-7b-chat-vocabless-f16.bin\n"
     ]
    }
   ],
   "source": [
    "fout = open(fname_out, \"wb\")\n",
    "fout.write(struct.pack(\"I\", 0x67676d64)) # magic: ggmd in hex\n",
    "fout.write(struct.pack(\"I\", 0)) # v1_no_vocab\n",
    "fout.write(struct.pack(\"I\", model.config.vocab_size))\n",
    "fout.write(struct.pack(\"I\", model.config.max_seq_len))\n",
    "fout.write(struct.pack(\"I\", model.config.n_layers))\n",
    "fout.write(struct.pack(\"I\", model.config.n_heads))\n",
    "fout.write(struct.pack(\"I\", model.config.d_model))\n",
    "fout.write(struct.pack(\"f\", model.config.attn_config['alibi_bias_max']))\n",
    "clip_qkv = model.config.attn_config['clip_qkv']\n",
    "fout.write(struct.pack(\"f\",  clip_qkv if clip_qkv is not None else 0))\n",
    "fout.write(struct.pack(\"I\", ftype))\n",
    "list_vars = model.state_dict()\n",
    "# for name in list_vars.keys():\n",
    "#     print(name, list_vars[name].shape, list_vars[name].dtype)\n",
    "for name in list_vars.keys():\n",
    "    data = list_vars[name].squeeze().numpy()\n",
    "    print(\"Processing variable: \" + name + \" with shape: \", data.shape)\n",
    "\n",
    "    n_dims = len(data.shape);\n",
    "\n",
    "    # ftype == 0 -> float32, ftype == 1 -> float16\n",
    "    ftype_cur = 0;\n",
    "    if ftype != 0:\n",
    "        # Keep token embeddings in fp32\n",
    "        if name[-7:] == \".weight\" and n_dims == 2 and \".wte\" not in name:\n",
    "            print(\"  Converting to float16\")\n",
    "            data = data.astype(np.float16)\n",
    "            ftype_cur = 1\n",
    "        else:\n",
    "            print(\"  Converting to float32\")\n",
    "            data = data.astype(np.float32)\n",
    "            ftype_cur = 0\n",
    "    else:\n",
    "        if data.dtype != np.float32:\n",
    "            print(\"  Converting to float32\")\n",
    "            data = data.astype(np.float32)\n",
    "            ftype_cur = 0\n",
    "\n",
    "    # header\n",
    "    str = name.encode('utf-8')\n",
    "    fout.write(struct.pack(\"iii\", n_dims, len(str), ftype_cur))\n",
    "    for i in range(n_dims):\n",
    "        fout.write(struct.pack(\"i\", data.shape[n_dims - 1 - i]))\n",
    "    fout.write(str);\n",
    "\n",
    "    # data\n",
    "    data.tofile(fout)\n",
    "fout.close()\n",
    "print(fname_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
